{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Statistical Analysis of BioMoQA Classification Models\n",
        "\n",
        "This notebook performs comprehensive statistical analysis comparing:\n",
        "1. BERT model configurations (BCE vs focal, with/without title, different negative sampling)\n",
        "2. Best BERT model vs baseline models (SVM, Random Forest)\n",
        "3. Friedman + Nemenyi statistical significance tests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import friedmanchisquare\n",
        "import scikit_posthocs as sp\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all datasets\n",
        "bert_without_title = pd.read_csv('../results/biomoqa/metrics/binary_metrics.csv')\n",
        "bert_with_title = pd.read_csv('../results/biomoqa/metrics/results.csv')\n",
        "svm_data = pd.read_csv('../results/biomoqa/metrics/svm_metrics.csv')\n",
        "rf_data = pd.read_csv('../results/biomoqa/metrics/random_forest_metrics.csv')\n",
        "\n",
        "print(\"Dataset shapes:\")\n",
        "print(f\"BERT without title: {bert_without_title.shape}\")\n",
        "print(f\"BERT with title: {bert_with_title.shape}\")\n",
        "print(f\"SVM: {svm_data.shape}\")\n",
        "print(f\"Random Forest: {rf_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine BERT datasets and add configuration labels\n",
        "bert_without_title['config'] = bert_without_title.apply(\n",
        "    lambda row: f\"{row['loss_type']}_no_title_{int(row['nb_added_negs'])}\", axis=1\n",
        ")\n",
        "bert_with_title['config'] = bert_with_title.apply(\n",
        "    lambda row: f\"{row['loss_type']}_with_title_{int(row['nb_added_negs'])}\", axis=1\n",
        ")\n",
        "\n",
        "# Combine all BERT data\n",
        "all_bert_data = pd.concat([bert_without_title, bert_with_title], ignore_index=True)\n",
        "\n",
        "# Filter out ensemble models for cleaner analysis\n",
        "all_bert_data = all_bert_data[all_bert_data['model_name'] != 'Ensemble']\n",
        "\n",
        "print(\"\\nUnique BERT configurations:\")\n",
        "print(all_bert_data['config'].unique())\n",
        "print(\"\\nUnique BERT models:\")\n",
        "print(all_bert_data['model_name'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Box Plot Comparisons by Configuration\n",
        "\n",
        "### 2.1 BERT Models by Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create box plots for different BERT configurations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "metrics = ['f1', 'roc_auc', 'accuracy', 'MCC']\n",
        "metric_names = ['F1 Score', 'ROC AUC', 'Accuracy', 'MCC']\n",
        "\n",
        "for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Create box plot\n",
        "    box_data = []\n",
        "    labels = []\n",
        "    \n",
        "    for config in sorted(all_bert_data['config'].unique()):\n",
        "        config_data = all_bert_data[all_bert_data['config'] == config][metric].dropna()\n",
        "        if len(config_data) > 0:\n",
        "            box_data.append(config_data)\n",
        "            labels.append(config.replace('_', '\\n'))\n",
        "    \n",
        "    bp = ax.boxplot(box_data, labels=labels, patch_artist=True)\n",
        "    \n",
        "    # Color the boxes\n",
        "    colors = sns.color_palette(\"husl\", len(box_data))\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.7)\n",
        "    \n",
        "    ax.set_title(f'{name} by Configuration', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel(name, fontsize=12)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('BERT Model Performance by Configuration', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 2.2 BERT Models by Individual Model Type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plot comparing different BERT model architectures\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    sns.boxplot(data=all_bert_data, x='model_name', y=metric, ax=ax)\n",
        "    ax.set_title(f'{name} by Model Architecture', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel(name, fontsize=12)\n",
        "    ax.set_xlabel('Model Architecture', fontsize=12)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('BERT Model Performance by Architecture', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Best Model Identification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best models for each configuration\n",
        "def find_best_models_by_config(data, metric='f1'):\n",
        "    \"\"\"Find best model for each configuration based on mean performance\"\"\"\n",
        "    best_models = {}\n",
        "    \n",
        "    for config in data['config'].unique():\n",
        "        config_data = data[data['config'] == config]\n",
        "        \n",
        "        # Calculate mean performance for each model in this config\n",
        "        model_means = config_data.groupby('model_name')[metric].mean()\n",
        "        best_model = model_means.idxmax()\n",
        "        best_score = model_means.max()\n",
        "        \n",
        "        best_models[config] = {\n",
        "            'model': best_model,\n",
        "            'mean_score': best_score,\n",
        "            'std_score': config_data[config_data['model_name'] == best_model][metric].std(),\n",
        "            'data': config_data[config_data['model_name'] == best_model]\n",
        "        }\n",
        "    \n",
        "    return best_models\n",
        "\n",
        "# Find best BERT models by configuration\n",
        "best_bert_by_config = find_best_models_by_config(all_bert_data, 'f1')\n",
        "\n",
        "print(\"Best BERT models by configuration (based on F1 score):\")\n",
        "print(\"=\" * 60)\n",
        "for config, info in best_bert_by_config.items():\n",
        "    print(f\"{config:25} | {info['model']:20} | F1: {info['mean_score']:.4f} ± {info['std_score']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find overall best BERT model\n",
        "overall_best_score = 0\n",
        "overall_best_config = None\n",
        "overall_best_model = None\n",
        "\n",
        "for config, info in best_bert_by_config.items():\n",
        "    if info['mean_score'] > overall_best_score:\n",
        "        overall_best_score = info['mean_score']\n",
        "        overall_best_config = config\n",
        "        overall_best_model = info['model']\n",
        "\n",
        "print(f\"\\nOverall best BERT model:\")\n",
        "print(f\"Configuration: {overall_best_config}\")\n",
        "print(f\"Model: {overall_best_model}\")\n",
        "print(f\"Mean F1 Score: {overall_best_score:.4f}\")\n",
        "\n",
        "# Get the best BERT model data\n",
        "best_bert_data = best_bert_by_config[overall_best_config]['data']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best SVM and Random Forest models\n",
        "def find_best_baseline_model(data, model_type, metric='f1'):\n",
        "    \"\"\"Find best baseline model configuration\"\"\"\n",
        "    if model_type == 'SVM':\n",
        "        # Group by kernel and other parameters\n",
        "        group_cols = ['kernel']\n",
        "    else:  # Random Forest\n",
        "        # Group by criterion and num_trees\n",
        "        group_cols = ['criterion', 'num_trees']\n",
        "    \n",
        "    # Calculate mean performance for each configuration\n",
        "    config_means = data.groupby(group_cols)[metric].mean()\n",
        "    best_config = config_means.idxmax()\n",
        "    best_score = config_means.max()\n",
        "    \n",
        "    # Get data for best configuration\n",
        "    if model_type == 'SVM':\n",
        "        best_data = data[data['kernel'] == best_config]\n",
        "        config_str = f\"kernel={best_config}\"\n",
        "    else:\n",
        "        best_data = data[(data['criterion'] == best_config[0]) & \n",
        "                        (data['num_trees'] == best_config[1])]\n",
        "        config_str = f\"criterion={best_config[0]}, num_trees={best_config[1]}\"\n",
        "    \n",
        "    return {\n",
        "        'config': config_str,\n",
        "        'mean_score': best_score,\n",
        "        'std_score': best_data[metric].std(),\n",
        "        'data': best_data\n",
        "    }\n",
        "\n",
        "# Find best baseline models\n",
        "best_svm = find_best_baseline_model(svm_data, 'SVM', 'f1')\n",
        "best_rf = find_best_baseline_model(rf_data, 'Random Forest', 'f1')\n",
        "\n",
        "print(\"\\nBest baseline models (based on F1 score):\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"SVM     | {best_svm['config']:30} | F1: {best_svm['mean_score']:.4f} ± {best_svm['std_score']:.4f}\")\n",
        "print(f\"RF      | {best_rf['config']:30} | F1: {best_rf['mean_score']:.4f} ± {best_rf['std_score']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Best Model vs Baselines Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare best BERT model with best baselines\n",
        "comparison_data = {\n",
        "    'Model': [],\n",
        "    'F1': [],\n",
        "    'ROC_AUC': [],\n",
        "    'Accuracy': [],\n",
        "    'MCC': [],\n",
        "    'Fold': []\n",
        "}\n",
        "\n",
        "# Add best BERT data\n",
        "for _, row in best_bert_data.iterrows():\n",
        "    comparison_data['Model'].append(f'BERT\\n({overall_best_model})')\n",
        "    comparison_data['F1'].append(row['f1'])\n",
        "    comparison_data['ROC_AUC'].append(row['roc_auc'])\n",
        "    comparison_data['Accuracy'].append(row['accuracy'])\n",
        "    comparison_data['MCC'].append(row['MCC'])\n",
        "    comparison_data['Fold'].append(row['fold'])\n",
        "\n",
        "# Add best SVM data\n",
        "for _, row in best_svm['data'].iterrows():\n",
        "    comparison_data['Model'].append('SVM')\n",
        "    comparison_data['F1'].append(row['f1'])\n",
        "    comparison_data['ROC_AUC'].append(row['roc_auc'] if 'roc_auc' in row else 0)\n",
        "    comparison_data['Accuracy'].append(row['accuracy'])\n",
        "    comparison_data['MCC'].append(row['MCC'])\n",
        "    comparison_data['Fold'].append(row['fold'])\n",
        "\n",
        "# Add best RF data\n",
        "for _, row in best_rf['data'].iterrows():\n",
        "    comparison_data['Model'].append('Random Forest')\n",
        "    comparison_data['F1'].append(row['f1'])\n",
        "    comparison_data['ROC_AUC'].append(row['roc_auc'] if 'roc_auc' in row else 0)\n",
        "    comparison_data['Accuracy'].append(row['accuracy'])\n",
        "    comparison_data['MCC'].append(row['MCC'])\n",
        "    comparison_data['Fold'].append(row['fold'])\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "comparison_metrics = ['F1', 'ROC_AUC', 'Accuracy', 'MCC']\n",
        "for i, metric in enumerate(comparison_metrics):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    sns.boxplot(data=comparison_df, x='Model', y=metric, ax=ax)\n",
        "    ax.set_title(f'{metric} Comparison: Best Models', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel(metric, fontsize=12)\n",
        "    ax.set_xlabel('Model Type', fontsize=12)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add mean values as text\n",
        "    means = comparison_df.groupby('Model')[metric].mean()\n",
        "    for j, (model, mean_val) in enumerate(means.items()):\n",
        "        ax.text(j, mean_val, f'{mean_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.suptitle('Performance Comparison: Best BERT vs Best Baselines', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Statistical Significance Testing\n",
        "\n",
        "### 5.1 Friedman Test + Nemenyi Post-hoc for BERT Configurations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for Friedman test - best model from each configuration\n",
        "friedman_data = []\n",
        "config_names = []\n",
        "fold_info = []\n",
        "\n",
        "# Get performance for each fold for the best model from each configuration\n",
        "for config, info in best_bert_by_config.items():\n",
        "    config_data = info['data'].sort_values('fold')\n",
        "    f1_scores = config_data['f1'].values\n",
        "    \n",
        "    if len(f1_scores) >= 4:  # Ensure we have data for all folds\n",
        "        friedman_data.append(f1_scores)\n",
        "        config_names.append(config)\n",
        "        fold_info.append(config_data[['fold', 'f1']].copy())\n",
        "\n",
        "print(f\"Configurations included in Friedman test: {len(config_names)}\")\n",
        "for i, name in enumerate(config_names):\n",
        "    print(f\"{i+1:2d}. {name} (n={len(friedman_data[i])})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Friedman test\n",
        "if len(friedman_data) >= 3:\n",
        "    friedman_stat, friedman_p = friedmanchisquare(*friedman_data)\n",
        "    \n",
        "    print(\"\\nFriedman Test Results:\")\n",
        "    print(\"=\" * 30)\n",
        "    print(f\"Test statistic: {friedman_stat:.4f}\")\n",
        "    print(f\"P-value: {friedman_p:.6f}\")\n",
        "    print(f\"Significant (α=0.05): {'Yes' if friedman_p < 0.05 else 'No'}\")\n",
        "    \n",
        "    # If significant, perform Nemenyi post-hoc test\n",
        "    if friedman_p < 0.05:\n",
        "        print(\"\\nPerforming Nemenyi post-hoc test...\")\n",
        "        \n",
        "        # Prepare data for post-hoc test\n",
        "        posthoc_data = np.array(friedman_data).T  # Transpose for correct format\n",
        "        \n",
        "        # Perform Nemenyi test\n",
        "        nemenyi_result = sp.posthoc_nemenyi_friedman(posthoc_data)\n",
        "        \n",
        "        # Create a more readable result with configuration names\n",
        "        nemenyi_result.index = [name.replace('_', ' ') for name in config_names]\n",
        "        nemenyi_result.columns = [name.replace('_', ' ') for name in config_names]\n",
        "        \n",
        "        print(\"\\nNemenyi Post-hoc Test Results (p-values):\")\n",
        "        print(nemenyi_result.round(4))\n",
        "        \n",
        "        # Visualize the results\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(nemenyi_result, annot=True, cmap='RdYlBu_r', center=0.05, \n",
        "                   square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
        "        plt.title('Nemenyi Post-hoc Test Results\\n(p-values for pairwise comparisons)', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Configuration', fontsize=12)\n",
        "        plt.ylabel('Configuration', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Count significant differences\n",
        "        significant_pairs = 0\n",
        "        total_pairs = 0\n",
        "        for i in range(len(config_names)):\n",
        "            for j in range(i+1, len(config_names)):\n",
        "                p_val = nemenyi_result.iloc[i, j]\n",
        "                if p_val < 0.05:\n",
        "                    significant_pairs += 1\n",
        "                total_pairs += 1\n",
        "        \n",
        "        print(f\"\\nSignificant pairwise differences: {significant_pairs}/{total_pairs}\")\n",
        "        print(f\"Percentage of significant pairs: {(significant_pairs/total_pairs)*100:.1f}%\")\n",
        "    \n",
        "else:\n",
        "    print(\"Not enough configurations for Friedman test (minimum 3 required)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 5.2 Statistical Test for Best Models (BERT vs Baselines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for comparing best models\n",
        "best_models_comparison = {\n",
        "    'BERT': best_bert_data['f1'].values,\n",
        "    'SVM': best_svm['data']['f1'].values,\n",
        "    'Random Forest': best_rf['data']['f1'].values\n",
        "}\n",
        "\n",
        "# Ensure all have the same number of samples (folds)\n",
        "min_samples = min(len(scores) for scores in best_models_comparison.values())\n",
        "for model_type in best_models_comparison:\n",
        "    best_models_comparison[model_type] = best_models_comparison[model_type][:min_samples]\n",
        "\n",
        "print(\"Best Models Comparison Data:\")\n",
        "for model_type, scores in best_models_comparison.items():\n",
        "    print(f\"{model_type:15} | Mean F1: {np.mean(scores):.4f} ± {np.std(scores):.4f} (n={len(scores)})\")\n",
        "\n",
        "# Perform Friedman test for best models\n",
        "if len(best_models_comparison) >= 3 and min_samples >= 3:\n",
        "    friedman_stat_best, friedman_p_best = friedmanchisquare(\n",
        "        *list(best_models_comparison.values())\n",
        "    )\n",
        "    \n",
        "    print(\"\\nFriedman Test Results (Best Models):\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Test statistic: {friedman_stat_best:.4f}\")\n",
        "    print(f\"P-value: {friedman_p_best:.6f}\")\n",
        "    print(f\"Significant (α=0.05): {'Yes' if friedman_p_best < 0.05 else 'No'}\")\n",
        "    \n",
        "    if friedman_p_best < 0.05:\n",
        "        # Post-hoc test for best models\n",
        "        posthoc_data_best = np.array(list(best_models_comparison.values())).T\n",
        "        nemenyi_result_best = sp.posthoc_nemenyi_friedman(posthoc_data_best)\n",
        "        \n",
        "        nemenyi_result_best.index = list(best_models_comparison.keys())\n",
        "        nemenyi_result_best.columns = list(best_models_comparison.keys())\n",
        "        \n",
        "        print(\"\\nNemenyi Post-hoc Test Results (Best Models):\")\n",
        "        print(nemenyi_result_best.round(4))\n",
        "        \n",
        "        # Visualize\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(nemenyi_result_best, annot=True, cmap='RdYlBu_r', center=0.05,\n",
        "                   square=True, linewidths=1, cbar_kws={\"shrink\": .8})\n",
        "        plt.title('Nemenyi Test: Best Models Comparison\\n(p-values)', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"\\nInsufficient data for statistical test of best models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Summary and Rankings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary table of all configurations\n",
        "summary_data = []\n",
        "\n",
        "# Add BERT configurations\n",
        "for config, info in best_bert_by_config.items():\n",
        "    summary_data.append({\n",
        "        'Model Type': 'BERT',\n",
        "        'Configuration': config,\n",
        "        'Best Model': info['model'],\n",
        "        'Mean F1': info['mean_score'],\n",
        "        'Std F1': info['std_score'],\n",
        "        'Mean ROC AUC': info['data']['roc_auc'].mean(),\n",
        "        'Mean Accuracy': info['data']['accuracy'].mean(),\n",
        "        'Mean MCC': info['data']['MCC'].mean()\n",
        "    })\n",
        "\n",
        "# Add baseline models\n",
        "summary_data.append({\n",
        "    'Model Type': 'SVM',\n",
        "    'Configuration': best_svm['config'],\n",
        "    'Best Model': 'SVM',\n",
        "    'Mean F1': best_svm['mean_score'],\n",
        "    'Std F1': best_svm['std_score'],\n",
        "    'Mean ROC AUC': best_svm['data']['roc_auc'].mean() if 'roc_auc' in best_svm['data'] else 0,\n",
        "    'Mean Accuracy': best_svm['data']['accuracy'].mean(),\n",
        "    'Mean MCC': best_svm['data']['MCC'].mean()\n",
        "})\n",
        "\n",
        "summary_data.append({\n",
        "    'Model Type': 'Random Forest',\n",
        "    'Configuration': best_rf['config'],\n",
        "    'Best Model': 'Random Forest',\n",
        "    'Mean F1': best_rf['mean_score'],\n",
        "    'Std F1': best_rf['std_score'],\n",
        "    'Mean ROC AUC': best_rf['data']['roc_auc'].mean() if 'roc_auc' in best_rf['data'] else 0,\n",
        "    'Mean Accuracy': best_rf['data']['accuracy'].mean(),\n",
        "    'Mean MCC': best_rf['data']['MCC'].mean()\n",
        "})\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df = summary_df.sort_values('Mean F1', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 120)\n",
        "print(\"FINAL RANKING - ALL MODELS AND CONFIGURATIONS\")\n",
        "print(\"=\" * 120)\n",
        "print(summary_df.round(4).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Highlight top 3\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"TOP 3 PERFORMERS:\")\n",
        "print(\"=\" * 50)\n",
        "for i in range(min(3, len(summary_df))):\n",
        "    row = summary_df.iloc[i]\n",
        "    print(f\"{i+1}. {row['Model Type']} - {row['Configuration']}\")\n",
        "    print(f\"   Model: {row['Best Model']}\")\n",
        "    print(f\"   F1: {row['Mean F1']:.4f} ± {row['Std F1']:.4f}\")\n",
        "    print(f\"   Accuracy: {row['Mean Accuracy']:.4f}\")\n",
        "    print(f\"   MCC: {row['Mean MCC']:.4f}\")\n",
        "    print()\n",
        "\n",
        "print(\"STATISTICAL ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1. BEST OVERALL PERFORMANCE:\")\n",
        "best_overall = summary_df.iloc[0]\n",
        "print(f\"   Model: {best_overall['Model Type']} - {best_overall['Best Model']}\")\n",
        "print(f\"   Configuration: {best_overall['Configuration']}\")\n",
        "print(f\"   F1 Score: {best_overall['Mean F1']:.4f} ± {best_overall['Std F1']:.4f}\")\n",
        "\n",
        "print(\"\\n2. CONFIGURATION ANALYSIS:\")\n",
        "bert_configs = summary_df[summary_df['Model Type'] == 'BERT']\n",
        "best_bert_config = bert_configs.iloc[0]\n",
        "print(f\"   Best BERT configuration: {best_bert_config['Configuration']}\")\n",
        "print(f\"   Best BERT model: {best_bert_config['Best Model']}\")\n",
        "\n",
        "# Performance improvement over baselines\n",
        "svm_f1 = summary_df[summary_df['Model Type'] == 'SVM']['Mean F1'].iloc[0]\n",
        "rf_f1 = summary_df[summary_df['Model Type'] == 'Random Forest']['Mean F1'].iloc[0]\n",
        "best_bert_f1 = best_bert_config['Mean F1']\n",
        "\n",
        "svm_improvement = ((best_bert_f1 - svm_f1) / svm_f1) * 100\n",
        "rf_improvement = ((best_bert_f1 - rf_f1) / rf_f1) * 100\n",
        "\n",
        "print(\"\\n3. PERFORMANCE IMPROVEMENTS:\")\n",
        "print(f\"   Best BERT vs SVM: +{svm_improvement:.2f}% improvement\")\n",
        "print(f\"   Best BERT vs Random Forest: +{rf_improvement:.2f}% improvement\")\n",
        "\n",
        "print(\"\\n4. RECOMMENDATIONS:\")\n",
        "print(f\"   • Use {best_overall['Best Model']} with {best_overall['Configuration']}\")\n",
        "print(f\"   • Expected F1 performance: {best_overall['Mean F1']:.4f} ± {best_overall['Std F1']:.4f}\")\n",
        "print(f\"   • This configuration shows superior performance over baseline methods\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
