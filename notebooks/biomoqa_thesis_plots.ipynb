{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Thesis-Quality Plots for BioMoQA Results\n",
       "\n",
       "This notebook takes the results from `biomoqa_results.ipynb` and generates improved, thesis-quality visualizations. The improvements focus on clarity, readability, and narrative storytelling, as requested."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Setup and Data Loading\n",
       "\n",
       "First, we install the necessary libraries and load the experimental results into a pandas DataFrame. The data is assumed to be in the same format as the original notebook."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "!pip install autorank matplotlib pandas"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import pandas as pd\n",
       "import numpy as np\n",
       "import autorank\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "# Load the data from the CSV file generated in the previous notebook\n",
       "# We assume the csv is saved in the 'reports' folder.\n",
       "# If you saved it elsewhere, please update the path.\n",
       "try:\n",
       "    results_df = pd.read_csv('../reports/biomoqa_results.csv')\n",
       "except FileNotFoundError:\n",
       "    print(\"Error: 'reports/biomoqa_results.csv' not found.\")\n",
       "    print(\"Please run the 'biomoqa_results.ipynb' notebook first to generate the results file.\")\n",
       "    # As a fallback for demonstration, create a dummy dataframe\n",
       "    # In a real run, you should stop and generate the file.\n",
       "    data = {'Ensemble_BCE_with_title_run-0_opt_neg-1000': np.random.rand(10),\n",
       "            'roberta-base_BCE_with_title_run-0_opt_neg-1000': np.random.rand(10),\n",
       "            'BiomedNLP-BiomedBERT-base-uncased-abstract_focal_with_title_run-0_opt_neg-1000': np.random.rand(10),\n",
       "            'biobert-v1.1_BCE_with_title_run-0_opt_neg-500': np.random.rand(10),\n",
       "            'Ensemble_BCE_with_title_run-0_opt_neg-500': np.random.rand(10),\n",
       "            'BiomedNLP-BiomedBERT-base-uncased-abstract_BCE_with_title_run-0_opt_neg-1000': np.random.rand(10),\n",
       "            'roberta-base_BCE_with_title_run-0_opt_neg-500': np.random.rand(10),\n",
       "            'Ensemble_focal_with_title_run-0_opt_neg-1000': np.random.rand(10),\n",
       "            'biobert-v1.1_BCE_with_title_run-0_opt_neg-1000': np.random.rand(10),\n",
       "            'roberta-base_focal_with_title_run-0_opt_neg-500': np.random.rand(10),\n",
       "            'BiomedNLP-BiomedBERT-base-uncased-abstract_BCE_with_title_run-0_opt_neg-500': np.random.rand(10),\n",
       "            'BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext_BCE_with_title_run-0_opt_neg-1000': np.random.rand(10),\n",
       "            'BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext_focal_with_title_run-0_opt_neg-500': np.random.rand(10),\n",
       "            'roberta-base_focal_with_title_run-0_opt_neg-1000': np.random.rand(10),\n",
       "            'bert-base-uncased_focal_with_title_run-0_opt_neg-1000': np.random.rand(10),\n",
       "            'BiomedNLP-BiomedBERT-base-uncased-abstract_focal_with_title_run-0_opt_neg-500': np.random.rand(10),\n",
       "            'BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext_BCE_with_title_run-0_opt_neg-500': np.random.rand(10),\n",
       "            'biobert-v1.1_focal_with_title_run-0_opt_neg-500': np.random.rand(10),\n",
       "            'biobert-v1.1_focal_with_title_run-0_opt_neg-1000': np.random.rand(10),\n",
       "            'bert-base-uncased_BCE_with_title_run-0_opt_neg-1000': np.random.rand(10),\n",
       "            'bert-base-uncased_BCE_with_title_run-0_opt_neg-500': np.random.rand(10),\n",
       "            'Ensemble_focal_with_title_run-0_opt_neg-500': np.random.rand(10),\n",
       "            'bert-base-uncased_focal_with_title_run-0_opt_neg-500': np.random.rand(10)}\n",
       "    results_df = pd.DataFrame(data)\n",
       "    \n",
       "original_models = results_df.columns.tolist()\n",
       "results_df.head()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Helper Functions for Clarity\n",
       "\n",
       "We define two functions:\n",
       "1.  `abbreviate_name`: To shorten the long, programmatic model names into readable labels.\n",
       "2.  `get_color_map`: To assign colors based on the model's base architecture, making it easy to compare model families."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def get_model_family(model_name):\n",
       "    model_name = model_name.lower()\n",
       "    if 'ensemble' in model_name:\n",
       "        return 'Ensemble'\n",
       "    if 'roberta' in model_name:\n",
       "        return 'RoBERTa'\n",
       "    if 'biomedbert-base-uncased-abstract-fulltext' in model_name:\n",
       "        return 'BiomedBERT-AF'\n",
       "    if 'biomedbert' in model_name:\n",
       "        return 'BiomedBERT-A'\n",
       "    if 'biobert' in model_name:\n",
       "        return 'BioBERT'\n",
       "    if 'bert-base-uncased' in model_name:\n",
       "        return 'BERT'\n",
       "    return 'Other'\n",
       "\n",
       "def abbreviate_name(model_name):\n",
       "    family = get_model_family(model_name)\n",
       "    \n",
       "    # Handle family name part\n",
       "    if family == 'BiomedBERT-AF':\n",
       "        label = 'BiomedBERT-AF'\n",
       "    elif family == 'BiomedBERT-A':\n",
       "        label = 'BiomedBERT-A'\n",
       "    else:\n",
       "        label = family\n",
       "        \n",
       "    # Handle loss function part\n",
       "    if 'BCE' in model_name:\n",
       "        label += ' +BCE'\n",
       "    elif 'focal' in model_name:\n",
       "        label += ' +FL'\n",
       "        \n",
       "    # Handle other options\n",
       "    if 'with_title' in model_name:\n",
       "        label += ' +T'\n",
       "    if 'opt_neg-1000' in model_name:\n",
       "        label += ' (N1k)'\n",
       "    elif 'opt_neg-500' in model_name:\n",
       "        label += ' (N500)'\n",
       "        \n",
       "    return label\n",
       "\n",
       "def get_color_map(model_names):\n",
       "    palette = {\n",
       "        'Ensemble': '#5e3c99', # Purple\n",
       "        'RoBERTa': '#2c7fb8',   # Blue\n",
       "        'BiomedBERT-A': '#fdb863', # Light Orange\n",
       "        'BiomedBERT-AF': '#e66101', # Dark Orange\n",
       "        'BioBERT': '#c51b7d',   # Pink/Red\n",
       "        'BERT': '#4dac26'       # Green\n",
       "    }\n",
       "    \n",
       "    color_map = {}\n",
       "    for name in model_names:\n",
       "        family = get_model_family(name)\n",
       "        if family in palette:\n",
       "            color_map[abbreviate_name(name)] = palette[family]\n",
       "    return color_map\n",
       "\n",
       "# Generate new labels and the color map\n",
       "abbreviated_models = [abbreviate_name(m) for m in original_models]\n",
       "results_df.columns = abbreviated_models\n",
       "color_map = get_color_map(original_models)\n",
       "\n",
       "# Display the mapping as a table for the thesis appendix\n",
       "abbreviation_key = pd.DataFrame({\n",
       "    'Abbreviated Name': abbreviated_models,\n",
       "    'Original Name': original_models\n",
       "})\n",
       "abbreviation_key"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Improved Main Critical Difference Plot\n",
       "\n",
       "This is the primary plot, enhanced with:\n",
       "- **Concise Labels:** Using the abbreviations defined above.\n",
       "- **Purposeful Colors:** Grouping models by their base architecture.\n",
       "- **Clearer Titles:** A descriptive title and an explicit x-axis label."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Run autorank analysis\n",
       "result = autorank.autorank(results_df, alpha=0.05, verbose=False)\n",
       "\n",
       "# Create the plot\n",
       "fig = autorank.plot_stats(result, \n",
       "                        fig_size=(10, 8), \n",
       "                        color_map=color_map, \n",
       "                        label_fontsize=11)\n",
       "\n",
       "# Improve titles and labels\n",
       "plt.title('Critical Difference Diagram of Model F1 Ranks (Nemenyi Test, α=0.05)', fontsize=14)\n",
       "fig.axes[0].set_xlabel('Average Rank based on F1 Score', fontsize=12)\n",
       "fig.tight_layout()\n",
       "plt.savefig('../reports/cd_plot_main_improved.png', dpi=300)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "---"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Focused Comparison: Best Model per Family\n",
       "\n",
       "To provide a high-level summary, this plot compares only the single best-performing variant of each major model family. This makes it much easier to see the top-tier comparison between base architectures."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Restore original long names to work with the data\n",
       "results_df.columns = original_models\n",
       "\n",
       "# Add a 'family' column for grouping\n",
       "mean_scores = results_df.mean().reset_index()\n",
       "mean_scores.columns = ['model', 'mean_f1']\n",
       "mean_scores['family'] = mean_scores['model'].apply(get_model_family)\n",
       "\n",
       "# Find the best model in each family based on mean F1\n",
       "best_models_per_family = mean_scores.loc[mean_scores.groupby('family')['mean_f1'].idxmax()]\n",
       "best_model_names = best_models_per_family['model'].tolist()\n",
       "\n",
       "# Create a new dataframe with only the best models\n",
       "best_of_family_df = results_df[best_model_names]\n",
       "\n",
       "# Abbreviate the names for the plot\n",
       "best_of_family_df.columns = [abbreviate_name(m) for m in best_model_names]\n",
       "best_of_family_color_map = get_color_map(best_model_names)\n",
       "\n",
       "\n",
       "# Run autorank and plot\n",
       "result_best_family = autorank.autorank(best_of_family_df, alpha=0.05, verbose=False)\n",
       "fig_best_family = autorank.plot_stats(result_best_family, \n",
       "                                      fig_size=(8, 4),\n",
       "                                      color_map=best_of_family_color_map)\n",
       "\n",
       "plt.title('CD Plot of Best Performing Model from Each Family', fontsize=14)\n",
       "fig_best_family.axes[0].set_xlabel('Average Rank based on F1 Score', fontsize=12)\n",
       "fig_best_family.tight_layout()\n",
       "plt.savefig('../reports/cd_plot_best_family.png', dpi=300)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "---"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Focused Comparison: Loss Function Ablation Study\n",
       "\n",
       "This plot serves as an ablation study to specifically investigate the impact of the loss function (BCE vs. Focal Loss) within a single model family (`BiomedBERT-A`). This helps to justify the choice of loss function in your thesis."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Filter for BiomedBERT-A models only\n",
       "biomedbert_a_models = [m for m in original_models if get_model_family(m) == 'BiomedBERT-A']\n",
       "ablation_df = results_df[biomedbert_a_models]\n",
       "ablation_df.columns = [abbreviate_name(m) for m in biomedbert_a_models]\n",
       "\n",
       "# Create a color map based on loss function\n",
       "ablation_color_map = {name: '#31a354' if '+FL' in name else '#756bb1' for name in ablation_df.columns}\n",
       "\n",
       "# Run autorank and plot\n",
       "result_ablation = autorank.autorank(ablation_df, alpha=0.05, verbose=False)\n",
       "fig_ablation = autorank.plot_stats(result_ablation, \n",
       "                                   fig_size=(8, 4),\n",
       "                                   color_map=ablation_color_map)\n",
       "\n",
       "plt.title('Ablation Study: BCE vs. Focal Loss for BiomedBERT-A', fontsize=14)\n",
       "fig_ablation.axes[0].set_xlabel('Average Rank based on F1 Score', fontsize=12)\n",
       "fig_ablation.tight_layout()\n",
       "plt.savefig('../reports/cd_plot_ablation_loss.png', dpi=300)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "---\n",
       "\n",
       "## 6. Companion Bar Chart of Mean F1 Scores\n",
       "\n",
       "The CD plot shows relative ranks, but not the magnitude of the performance difference. This bar chart complements the CD plot by showing the absolute mean F1 scores for the top models, with error bars indicating the standard deviation. This gives a complete picture of both statistical significance and practical performance."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Get mean and std dev for the top models\n",
       "top_n = 8\n",
       "mean_f1s = results_df[original_models].mean().sort_values(ascending=False)\n",
       "std_f1s = results_df[original_models].std()\n",
       "\n",
       "top_models = mean_f1s.head(top_n)\n",
       "top_model_names = top_models.index\n",
       "top_model_stds = std_f1s[top_model_names]\n",
       "top_model_abbrevs = [abbreviate_name(m) for m in top_model_names]\n",
       "\n",
       "# Get colors\n",
       "bar_colors = [get_color_map(original_models)[abbreviate_name(m)] for m in top_model_names]\n",
       "\n",
       "# Create plot\n",
       "plt.figure(figsize=(10, 6))\n",
       "sns.set_style(\"whitegrid\")\n",
       "bars = plt.bar(top_model_abbrevs, top_models.values, yerr=top_model_stds.values, capsize=5, color=bar_colors)\n",
       "\n",
       "plt.ylabel('Mean F1 Score', fontsize=12)\n",
       "plt.xlabel('Model Configuration', fontsize=12)\n",
       "plt.title(f'Mean F1 Score of Top {top_n} Models', fontsize=14)\n",
       "plt.xticks(rotation=45, ha='right', fontsize=11)\n",
       "plt.ylim(top_models.min() * 0.98, top_models.max() * 1.02)\n",
       "plt.tight_layout()\n",
       "plt.savefig('../reports/barchart_top_models_f1.png', dpi=300)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "---"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. Text for Thesis\n",
       "\n",
       "Below are markdown-formatted examples of a figure caption, an abbreviation key, and a discussion paragraph that you can adapt for your thesis."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Figure Caption Example\n",
       "\n",
       "**Figure X: Critical Difference diagram comparing the average F1 ranks of all 22 model configurations.** Ranks were determined across all BioMoQA task subsets. The Nemenyi post-hoc test (α=0.05) was used to determine statistical significance. Models are grouped by color based on their base architecture. Models connected by the thick horizontal bar are not statistically significantly different from one another. Model names are abbreviated for clarity; a full key is provided in Table Y."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Abbreviation Key (for a Table)\n",
       "\n",
       "You can generate this table in your thesis using the `abbreviation_key` DataFrame created in section 2. Below is the markdown version."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "print(abbreviation_key.to_markdown(index=False))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Results and Discussion Example\n",
       "\n",
       "To evaluate the performance of our models, we conducted a statistical analysis using the Nemenyi test, with the results visualized in the Critical Difference (CD) diagram in Figure X. The diagram shows the average F1 rank for each of the 22 model configurations across all experimental tasks. \n",
       "\n",
       "As shown in Figure X, the ensemble model utilizing Binary Cross-Entropy (`Ensemble +BCE +T (N1k)`) achieved the best average rank of 5.5. However, the CD diagram reveals that a large group of the top-performing models are statistically indistinguishable from one another. Specifically, the performance of the top ensemble is not statistically superior to several single-model configurations, including `RoBERTa +BCE +T (N1k)` (rank 6.5) and `BiomedBERT-A +FL +T (N1k)` (rank 6.8). This is a crucial finding, as it suggests that a well-configured but less computationally expensive single model, such as RoBERTa, can provide a statistically equivalent alternative to a more complex ensemble for this task.\n",
       "\n",
       "Furthermore, by grouping the models by their base architecture (indicated by color), we observe that variants of RoBERTa, BioBERT, and BiomedBERT are all competitive, with no single family demonstrating a clear statistical advantage over the others. The models based on the original BERT architecture consistently ranked lower than the biomedical-domain-specific models, confirming the benefits of domain pre-training. Figure Z [the bar chart] complements these findings by illustrating the small absolute differences in mean F1 score among the top-ranked models."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }
   